{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Implemntation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import fnmatch\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from time import time\n",
    "from typing import List\n",
    "\n",
    "import tensorflow.keras\n",
    "import numpy\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from ClassWeightCalculator import ClassWeightCalculator\n",
    "from reporting import TelegramNotifier, GoogleSpreadsheetReporter, sklearn_reporting\n",
    "from reporting.TrainingHistoryPlotter import TrainingHistoryPlotter\n",
    "from datasets.TrainingDatasetProvider import TrainingDatasetProvider\n",
    "from datasets.DirectoryIteratorWithBoundingBoxes import DirectoryIteratorWithBoundingBoxes\n",
    "from models.ConfigurationFactory import ConfigurationFactory\n",
    "\n",
    "\n",
    "def train_model(dataset_directory: str, model_name: str, stroke_thicknesses: List[int],\n",
    "                width: int, height: int,\n",
    "                staff_line_vertical_offsets: List[int], training_minibatch_size: int,\n",
    "                optimizer: str, dynamic_learning_rate_reduction: bool, use_fixed_canvas: bool, datasets: List[str],\n",
    "                class_weights_balancing_method: str,\n",
    "                save_after_every_epoch: bool,\n",
    "                resume_from_checkpoint: str,\n",
    "                send_telegram_messages: bool):\n",
    "    image_dataset_directory = os.path.join(dataset_directory, \"images\")\n",
    "\n",
    "    bounding_boxes = None\n",
    "    bounding_boxes_cache = os.path.join(dataset_directory, \"bounding_boxes.txt\")\n",
    "\n",
    "    print(\"Loading configuration and data-readers...\")\n",
    "    start_time = time()\n",
    "\n",
    "    number_of_classes = len(os.listdir(os.path.join(image_dataset_directory, \"training\")))\n",
    "    training_configuration = ConfigurationFactory.get_configuration_by_name(model_name, optimizer, width, height,\n",
    "                                                                            training_minibatch_size, number_of_classes)\n",
    "    if training_configuration.performs_localization() and bounding_boxes is None:\n",
    "        # Try to unpickle\n",
    "        with open(bounding_boxes_cache, \"rb\") as cache:\n",
    "            bounding_boxes = pickle.load(cache)\n",
    "\n",
    "    if not training_configuration.performs_localization():\n",
    "        bounding_boxes = None\n",
    "\n",
    "    train_generator = ImageDataGenerator(rotation_range=training_configuration.rotation_range,\n",
    "                                         zoom_range=training_configuration.zoom_range\n",
    "                                         )\n",
    "    training_data_generator = DirectoryIteratorWithBoundingBoxes(\n",
    "        directory=os.path.join(image_dataset_directory, \"training\"),\n",
    "        image_data_generator=train_generator,\n",
    "        target_size=(training_configuration.input_image_rows,\n",
    "                     training_configuration.input_image_columns),\n",
    "        batch_size=training_configuration.training_minibatch_size,\n",
    "        bounding_boxes=bounding_boxes,\n",
    "    )\n",
    "    training_steps_per_epoch = np.math.ceil(training_data_generator.samples / training_data_generator.batch_size)\n",
    "\n",
    "    validation_generator = ImageDataGenerator()\n",
    "    validation_data_generator = DirectoryIteratorWithBoundingBoxes(\n",
    "        directory=os.path.join(image_dataset_directory, \"validation\"),\n",
    "        image_data_generator=validation_generator,\n",
    "        target_size=(\n",
    "            training_configuration.input_image_rows,\n",
    "            training_configuration.input_image_columns),\n",
    "        batch_size=training_configuration.training_minibatch_size,\n",
    "        bounding_boxes=bounding_boxes)\n",
    "    validation_steps_per_epoch = np.math.ceil(validation_data_generator.samples / validation_data_generator.batch_size)\n",
    "\n",
    "    test_generator = ImageDataGenerator()\n",
    "    test_data_generator = DirectoryIteratorWithBoundingBoxes(\n",
    "        directory=os.path.join(image_dataset_directory, \"test\"),\n",
    "        image_data_generator=test_generator,\n",
    "        target_size=(training_configuration.input_image_rows,\n",
    "                     training_configuration.input_image_columns),\n",
    "        batch_size=training_configuration.training_minibatch_size,\n",
    "        shuffle=False,\n",
    "        bounding_boxes=bounding_boxes)\n",
    "    test_steps_per_epoch = np.math.ceil(test_data_generator.samples / test_data_generator.batch_size)\n",
    "\n",
    "    model:Model = training_configuration.classifier()\n",
    "    model.summary()\n",
    "\n",
    "    print(\"Model {0} created.\".format(training_configuration.name()))\n",
    "\n",
    "    initial_epoch = 0\n",
    "    if resume_from_checkpoint:\n",
    "        # Try to parse epoch from checkpoint filename. Checkpoint files written by this program\n",
    "        # are of the form <start>_<configname>-<epoch>.h5.\n",
    "        # The regular expression assumes there are no dashes in configname, otherwise\n",
    "        # the initial epoch will just be 0. That is harmless unless you have parameters that\n",
    "        # depend on the epoch.\n",
    "        m = re.match('[\\d-]+_[^-]+-(\\d+).h5', resume_from_checkpoint)\n",
    "        if m and m.groups() and len(m.groups() == 1):\n",
    "            initial_epoch = int(m.groups()[0]) + 1\n",
    "        # This will not restore parameters that are adapted dynamically during training since\n",
    "        # afaik not all of them get saved to the model checkpoint.\n",
    "        model.load_weights(resume_from_checkpoint)\n",
    "        print(\"Model {0} weights loaded from checkpoint {1}. Training will resume from epoch {2}\".format(\n",
    "            training_configuration.name(),\n",
    "            resume_from_checkpoint,\n",
    "            initial_epoch))\n",
    "\n",
    "    print(training_configuration.summary())\n",
    "\n",
    "    start_of_training = datetime.date.today()\n",
    "\n",
    "    monitor_variable = 'val_accuracy'\n",
    "    if training_configuration.performs_localization():\n",
    "        monitor_variable = 'val_output_class_accuracy'\n",
    "\n",
    "    best_model_path = \"{0}_{1}\".format(start_of_training, training_configuration.name())\n",
    "    if save_after_every_epoch:\n",
    "        model_checkpoint = ModelCheckpoint(best_model_path + \"-{epoch:02d}.h5\", monitor=monitor_variable,\n",
    "                save_best_only=True, verbose=1, save_freq='epoch')\n",
    "    else:\n",
    "        model_checkpoint = ModelCheckpoint(best_model_path+\".h5\", monitor=monitor_variable,\n",
    "                save_best_only=True, verbose=1)\n",
    "    early_stop = EarlyStopping(monitor=monitor_variable,\n",
    "                               patience=training_configuration.number_of_epochs_before_early_stopping,\n",
    "                               verbose=1)\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor=monitor_variable,\n",
    "                                                patience=training_configuration.number_of_epochs_before_reducing_learning_rate,\n",
    "                                                verbose=1,\n",
    "                                                factor=training_configuration.learning_rate_reduction_factor,\n",
    "                                                min_lr=training_configuration.minimum_learning_rate)\n",
    "    tensorboard_callback = TensorBoard(\n",
    "            log_dir=\"./logs/{0}_{1}/\".format(start_of_training, training_configuration.name()))\n",
    "    if dynamic_learning_rate_reduction:\n",
    "        callbacks = [model_checkpoint, early_stop, tensorboard_callback, learning_rate_reduction]\n",
    "    else:\n",
    "        print(\"Learning-rate reduction on Plateau disabled\")\n",
    "        callbacks = [model_checkpoint, early_stop, tensorboard_callback]\n",
    "\n",
    "    class_weight_calculator = ClassWeightCalculator()\n",
    "    class_weights = class_weight_calculator.calculate_class_weights(image_dataset_directory,\n",
    "                                                                    method=class_weights_balancing_method,\n",
    "                                                                    class_indices=training_data_generator.class_indices)\n",
    "    if class_weights_balancing_method is not None:\n",
    "        print(\"Using {0} method for obtaining class weights to compensate for an unbalanced dataset.\".format(\n",
    "            class_weights_balancing_method))\n",
    "\n",
    "    print(\"Training on dataset...\")\n",
    "    history = model.fit(\n",
    "        x=training_data_generator,\n",
    "        steps_per_epoch=training_steps_per_epoch,\n",
    "        epochs=training_configuration.number_of_epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=validation_data_generator,\n",
    "        validation_steps=validation_steps_per_epoch,\n",
    "        class_weight=class_weights,\n",
    "        initial_epoch=initial_epoch\n",
    "    )\n",
    "\n",
    "    best_model = None\n",
    "    if not save_after_every_epoch:\n",
    "        print(\"Loading best model from check-point and testing...\")\n",
    "        best_model = tensorflow.keras.models.load_model(best_model_path + '.h5')\n",
    "    else:\n",
    "        print(\"Loading latest model from check-point and testing...\")\n",
    "        latest_mtime = 0\n",
    "        latest_file = None\n",
    "        pattern = best_model_path + '*.h5'\n",
    "        for f in os.listdir('.'):\n",
    "            if fnmatch.fnmatch(f, pattern):\n",
    "                fd = os.open(f, os.O_RDONLY)\n",
    "                st = os.fstat(fd)\n",
    "                mt = st.st_mtime\n",
    "                if not latest_file or mt > latest_mtime:\n",
    "                    latest_mtime = mt\n",
    "                    latest_file = f\n",
    "\n",
    "        print('latest model file is {0}'.format(latest_file))\n",
    "        best_model = tensorflow.keras.models.load_model(latest_file)\n",
    "\n",
    "    test_data_generator.reset()\n",
    "    file_names = test_data_generator.filenames\n",
    "    class_labels = os.listdir(os.path.join(image_dataset_directory, \"test\"))\n",
    "    # Notice that some classes have so few elements, that they are not present in the test-set and do not\n",
    "    # appear in the final report. To obtain the correct classes, we have to enumerate all non-empty class\n",
    "    # directories inside the test-folder and use them as labels\n",
    "    names_of_classes_with_test_data = [\n",
    "        class_name for class_name in class_labels\n",
    "        if os.listdir(os.path.join(image_dataset_directory, \"test\", class_name))]\n",
    "    true_classes = test_data_generator.classes\n",
    "    predictions = best_model.predict_generator(test_data_generator, steps=test_steps_per_epoch)\n",
    "    if training_configuration.performs_localization():\n",
    "        predicted_classes = numpy.argmax(predictions[0], axis=1)\n",
    "    else:\n",
    "        predicted_classes = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "    test_data_generator.reset()\n",
    "    evaluation = best_model.evaluate_generator(test_data_generator, steps=test_steps_per_epoch)\n",
    "    classification_accuracy = 0\n",
    "\n",
    "    print(\"Reporting classification statistics with micro average\")\n",
    "    report = sklearn_reporting.classification_report(true_classes, predicted_classes, digits=3,\n",
    "                                                     target_names=names_of_classes_with_test_data, average='micro')\n",
    "    print(report)\n",
    "\n",
    "    print(\"Reporting classification statistics with macro average\")\n",
    "    report = sklearn_reporting.classification_report(true_classes, predicted_classes, digits=3,\n",
    "                                                     target_names=names_of_classes_with_test_data, average='macro')\n",
    "    print(report)\n",
    "\n",
    "    print(\"Reporting classification statistics with weighted average\")\n",
    "    report = sklearn_reporting.classification_report(true_classes, predicted_classes, digits=3,\n",
    "                                                     target_names=names_of_classes_with_test_data, average='weighted'\n",
    "                                                     )\n",
    "    print(report)\n",
    "\n",
    "    indices_of_misclassified_files = [i for i, e in enumerate(true_classes - predicted_classes) if e != 0]\n",
    "    misclassified_files = [file_names[i] for i in indices_of_misclassified_files]\n",
    "    misclassified_files_actual_prediction_indices = [predicted_classes[i] for i in indices_of_misclassified_files]\n",
    "    misclassified_files_actual_prediction_classes = [class_labels[i] for i in\n",
    "                                                     misclassified_files_actual_prediction_indices]\n",
    "    print(\"Misclassified files:\")\n",
    "    for i in range(len(misclassified_files)):\n",
    "        print(\"\\t{0} is incorrectly classified as {1}\".format(misclassified_files[i],\n",
    "                                                              misclassified_files_actual_prediction_classes[i]))\n",
    "\n",
    "    for i in range(len(best_model.metrics_names)):\n",
    "        current_metric = best_model.metrics_names[i]\n",
    "        print(\"{0}: {1:.5f}\".format(current_metric, evaluation[i]))\n",
    "        if current_metric == 'accuracy' or current_metric == 'output_class_acc':\n",
    "            classification_accuracy = evaluation[i]\n",
    "    print(\"Total Accuracy: {0:0.5f}%\".format(classification_accuracy * 100))\n",
    "    print(\"Total Error: {0:0.5f}%\".format((1 - classification_accuracy) * 100))\n",
    "\n",
    "    end_time = time()\n",
    "    execution_time_in_seconds = round(end_time - start_time)\n",
    "    print(\"Execution time: {0:.1f}s\".format(end_time - start_time))\n",
    "\n",
    "    training_result_image = \"{1}_{0}_{2:.1f}p.png\".format(training_configuration.name(), start_of_training,\n",
    "                                                          classification_accuracy * 100)\n",
    "    TrainingHistoryPlotter.plot_history(history, training_result_image)\n",
    "\n",
    "    datasets_string = str.join(\",\", datasets)\n",
    "    notification_message = \"Training on {0} dataset with model {1} finished. \" \\\n",
    "                           \"Accuracy: {2:0.5f}%\".format(datasets_string, model_name, classification_accuracy * 100)\n",
    "    if send_telegram_messages:\n",
    "        TelegramNotifier.send_message_via_telegram(notification_message, training_result_image)\n",
    "    else:\n",
    "        print(notification_message)\n",
    "\n",
    "    dataset_size = training_data_generator.samples + validation_data_generator.samples + test_data_generator.samples\n",
    "    stroke_thicknesses_string = \",\".join(map(str, stroke_thicknesses))\n",
    "    staff_line_vertical_offsets_string = \",\".join(map(str, staff_line_vertical_offsets))\n",
    "    image_sizes = \"{0}x{1}px\".format(training_configuration.input_image_rows,\n",
    "                                     training_configuration.input_image_columns)\n",
    "    data_augmentation = \"{0}% zoom, {1}° rotation\".format(int(training_configuration.zoom_range * 100),\n",
    "                                                          training_configuration.rotation_range)\n",
    "    today = \"{0:02d}.{1:02d}.{2}\".format(start_of_training.day, start_of_training.month, start_of_training.year)\n",
    "    balancing_method = \"None\" if class_weights_balancing_method is None else class_weights_balancing_method\n",
    "\n",
    "    GoogleSpreadsheetReporter.append_result_to_spreadsheet(dataset_size=dataset_size, image_sizes=image_sizes,\n",
    "                                                           stroke_thicknesses=stroke_thicknesses_string,\n",
    "                                                           staff_lines=staff_line_vertical_offsets_string,\n",
    "                                                           model_name=model_name, data_augmentation=data_augmentation,\n",
    "                                                           optimizer=optimizer,\n",
    "                                                           early_stopping=training_configuration.number_of_epochs_before_early_stopping,\n",
    "                                                           reduction_patience=training_configuration.number_of_epochs_before_reducing_learning_rate,\n",
    "                                                           learning_rate_reduction_factor=training_configuration.learning_rate_reduction_factor,\n",
    "                                                           minibatch_size=training_minibatch_size,\n",
    "                                                           initialization=training_configuration.initialization,\n",
    "                                                           initial_learning_rate=training_configuration.get_initial_learning_rate(),\n",
    "                                                           accuracy=classification_accuracy,\n",
    "                                                           date=today,\n",
    "                                                           use_fixed_canvas=use_fixed_canvas,\n",
    "                                                           datasets=datasets_string,\n",
    "                                                           execution_time_in_seconds=execution_time_in_seconds,\n",
    "                                                           balancing_method=balancing_method)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "    parser.add_argument(\"--dataset_directory\", type=str, default=\"data\",\n",
    "                        help=\"The directory, that is used for storing the images during training\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"res_net_4\",\n",
    "                        help=\"The model used for training the network. Run ListAvailableConfigurations.ps1 or \"\n",
    "                             \"models/ConfigurationFactory.py to get a list of all available configurations\")\n",
    "\n",
    "    parser.add_argument(\"--use_existing_dataset_directory\", dest=\"delete_and_recreate_dataset_directory\",\n",
    "                        action='store_false',\n",
    "                        help=\"Whether to delete and recreate the dataset-directory (by downloading the appropriate \"\n",
    "                             \"files from the internet, extracting and generating images) or simply use whatever data \"\n",
    "                             \"currently is inside of that directory.\")\n",
    "    parser.set_defaults(delete_and_recreate_dataset_directory=True)\n",
    "\n",
    "    parser.add_argument(\"--minibatch_size\", default=16, type=int,\n",
    "                        help=\"Size of the minibatches for training, typically one of 8, 16, 32, 64 or 128\")\n",
    "    parser.add_argument(\"--optimizer\", default=\"Adadelta\", type=str,\n",
    "                        help=\"The optimizer used for the training, can be SGD, Adam or Adadelta\")\n",
    "\n",
    "    parser.add_argument(\"--no_dynamic_learning_rate_reduction\", dest=\"dynamic_learning_rate_reduction\",\n",
    "                        action=\"store_false\",\n",
    "                        help=\"True, if the learning rate should not be scheduled to be reduced on a plateau.\")\n",
    "    parser.set_defaults(dynamic_learning_rate_reduction=True)\n",
    "    parser.add_argument(\"--class_weights_balancing_method\", default=None, type=str,\n",
    "                        help=\"The optional weight balancing method for handling unbalanced datasets. If provided,\"\n",
    "                             \"valid choices are simple or skBalance. 'simple' uses 1/sqrt(#samples_per_class) as \"\n",
    "                             \"weights for samples from each class to compensate for classes that are underrepresented.\"\n",
    "                             \"'skBalance' uses the Python SkLearn module to calculate more sophisticated weights.\")\n",
    "    parser.add_argument(\"--telegram_messages\", dest=\"send_telegram_messages\", action=\"store_true\",\n",
    "                        help=\"Send messages via telegram\")\n",
    "    parser.set_defaults(send_telegram_messages=False)\n",
    "    parser.add_argument(\"--save_after_every_epoch\", dest=\"save_after_every_epoch\", action=\"store_true\",\n",
    "                        help=\"Write a checkpoint after every epoch\")\n",
    "    parser.set_defaults(save_after_every_epoch=False)\n",
    "    parser.add_argument(\"--resume_from_checkpoint\", dest=\"resume_from_checkpoint\", default=None, type=str,\n",
    "                        help=\"Load checkpoint from file specified.\")\n",
    "\n",
    "    TrainingDatasetProvider.add_arguments_for_training_dataset_provider(parser)\n",
    "\n",
    "    flags, unparsed = parser.parse_known_args()\n",
    "\n",
    "    offsets = []\n",
    "    if flags.offsets != \"\":\n",
    "        offsets = [int(o) for o in flags.offsets.split(',')]\n",
    "    stroke_thicknesses_for_generated_symbols = [int(s) for s in flags.stroke_thicknesses.split(',')]\n",
    "\n",
    "    if flags.datasets == \"\":\n",
    "        raise Exception(\"No dataset selected. Specify the dataset for the training via the --dataset parameter\")\n",
    "    datasets = flags.datasets.split(',')\n",
    "\n",
    "    if flags.delete_and_recreate_dataset_directory:\n",
    "        training_dataset_provider = TrainingDatasetProvider(flags.dataset_directory)\n",
    "        training_dataset_provider.recreate_and_prepare_datasets_for_training(\n",
    "            datasets=datasets, width=flags.width,\n",
    "            height=flags.height,\n",
    "            use_fixed_canvas=flags.use_fixed_canvas,\n",
    "            stroke_thicknesses_for_generated_symbols=stroke_thicknesses_for_generated_symbols,\n",
    "            staff_line_spacing=flags.staff_line_spacing,\n",
    "            staff_line_vertical_offsets=offsets,\n",
    "            random_position_on_canvas=flags.random_position_on_canvas)\n",
    "        training_dataset_provider.resize_all_images_to_fixed_size(flags.width, flags.height)\n",
    "        training_dataset_provider.split_dataset_into_training_validation_and_test_set()\n",
    "\n",
    "    train_model(dataset_directory=flags.dataset_directory,\n",
    "                model_name=flags.model_name,\n",
    "                stroke_thicknesses=stroke_thicknesses_for_generated_symbols,\n",
    "                width=flags.width,\n",
    "                height=flags.height,\n",
    "                staff_line_vertical_offsets=offsets,\n",
    "                training_minibatch_size=flags.minibatch_size,\n",
    "                optimizer=flags.optimizer,\n",
    "                dynamic_learning_rate_reduction=flags.dynamic_learning_rate_reduction,\n",
    "                use_fixed_canvas=flags.use_fixed_canvas,\n",
    "                datasets=datasets,\n",
    "                class_weights_balancing_method=flags.class_weights_balancing_method,\n",
    "                save_after_every_epoch=flags.save_after_every_epoch,\n",
    "                resume_from_checkpoint=flags.resume_from_checkpoint,\n",
    "                send_telegram_messages=flags.send_telegram_messages)\n",
    "\n",
    "    # To run in in python console\n",
    "    # dataset_directory = 'data'\n",
    "    # model_name = 'res_net_3_small'\n",
    "    # delete_and_recreate_dataset_directory = True\n",
    "    # stroke_thicknesses = [3]\n",
    "    # width = 96\n",
    "    # height = 192\n",
    "    # staff_line_vertical_offsets = None\n",
    "    # staff_line_spacing = 14\n",
    "    # training_minibatch_size = 32\n",
    "    # optimizer = 'Adadelta'\n",
    "    # dynamic_learning_rate_reduction = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Implmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# Import your custom modules\n",
    "from ClassWeightCalculator import ClassWeightCalculator\n",
    "from reporting import TelegramNotifier, sklearn_reporting\n",
    "from reporting.TrainingHistoryPlotter import TrainingHistoryPlotter\n",
    "from datasets.TrainingDatasetProvider import TrainingDatasetProvider\n",
    "from models.ConfigurationFactory import ConfigurationFactory\n",
    "\n",
    "\n",
    "def train_model(dataset_directory: str, model_name: str, stroke_thicknesses: List[int],\n",
    "                width: int, height: int,\n",
    "                staff_line_vertical_offsets: List[int], training_minibatch_size: int,\n",
    "                optimizer_name: str, dynamic_learning_rate_reduction: bool, use_fixed_canvas: bool, datasets: List[str],\n",
    "                class_weights_balancing_method: str,\n",
    "                save_after_every_epoch: bool,\n",
    "                resume_from_checkpoint: str):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    start_of_training = datetime.now()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Paths and Configurations\n",
    "    image_dataset_directory = os.path.join(dataset_directory, \"images\")\n",
    "    bounding_boxes_cache = os.path.join(dataset_directory, \"bounding_boxes.txt\")\n",
    "    bounding_boxes = None\n",
    "\n",
    "    print(\"Loading configuration and data-readers...\")\n",
    "    number_of_classes = len(os.listdir(os.path.join(image_dataset_directory, \"training\")))\n",
    "    training_configuration = ConfigurationFactory.get_configuration_by_name(\n",
    "        model_name, optimizer_name, width, height, training_minibatch_size, number_of_classes\n",
    "    )\n",
    "\n",
    "    if training_configuration.performs_localization():\n",
    "        # Load Bounding Boxes\n",
    "        if os.path.exists(bounding_boxes_cache):\n",
    "            with open(bounding_boxes_cache, \"rb\") as cache:\n",
    "                bounding_boxes = pickle.load(cache)\n",
    "        else:\n",
    "            print(\"Bounding boxes cache not found.\")\n",
    "    else:\n",
    "        bounding_boxes = None\n",
    "\n",
    "    # Data Augmentation and Preprocessing\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((height, width)),\n",
    "        transforms.RandomRotation(training_configuration.rotation_range),\n",
    "        transforms.RandomResizedCrop((height, width), scale=(1.0 - training_configuration.zoom_range, 1.0)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    val_test_transform = transforms.Compose([\n",
    "        transforms.Resize((height, width)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # Datasets and Loaders\n",
    "    if bounding_boxes is not None:\n",
    "        train_dataset = DirectoryIteratorWithBoundingBoxes(directory=os.path.join(image_dataset_directory, \"training\"),,\n",
    "                                                            bounding_boxes=bounding_boxes,\n",
    "                                                            target_size=(192, 96),\n",
    "                                                            transform=train_transform\n",
    "                                                        )\n",
    "        \n",
    "        val_dataset = DirectoryIteratorWithBoundingBoxes(directory=os.path.join(image_dataset_directory, \"validation\"),\n",
    "                                                            bounding_boxes=bounding_boxes,\n",
    "                                                            target_size=(192, 96),\n",
    "                                                            transform=val_test_transform\n",
    "                                                        )\n",
    "        test_dataset = DirectoryIteratorWithBoundingBoxes(directory=os.path.join(image_dataset_directory, \"test\"),\n",
    "                                                            bounding_boxes=bounding_boxes,\n",
    "                                                            target_size=(192, 96),\n",
    "                                                            transform=val_test_transform\n",
    "                                                        )\n",
    "    else:\n",
    "        train_dataset = ImageFolder(os.path.join(image_dataset_directory, \"training\"), transform=train_transform)\n",
    "        val_dataset = ImageFolder(os.path.join(image_dataset_directory, \"validation\"), transform=val_test_transform)\n",
    "        test_dataset = ImageFolder(os.path.join(image_dataset_directory, \"test\"), transform=val_test_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=training_minibatch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=training_minibatch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=training_minibatch_size, shuffle=False)\n",
    "\n",
    "    # Model, Optimizer, and Loss\n",
    "    model = training_configuration.classifier().to(device)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=training_configuration.get_initial_learning_rate())\n",
    "\n",
    "    # Learning Rate Scheduler\n",
    "    if dynamic_learning_rate_reduction:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max',\n",
    "                                                         patience=training_configuration.number_of_epochs_before_reducing_learning_rate,\n",
    "                                                         factor=training_configuration.learning_rate_reduction_factor,\n",
    "                                                         min_lr=training_configuration.minimum_learning_rate)\n",
    "    else:\n",
    "        scheduler = None\n",
    "\n",
    "    # Class Weights and Loss\n",
    "    class_weight_calculator = ClassWeightCalculator()\n",
    "    class_weights = class_weight_calculator.calculate_class_weights(\n",
    "        image_dataset_directory,\n",
    "        method=class_weights_balancing_method,\n",
    "        class_indices=train_dataset.class_to_idx\n",
    "    )\n",
    "    if class_weights_balancing_method is not None:\n",
    "        print(f\"Using {class_weights_balancing_method} method for obtaining class weights to compensate for an unbalanced dataset.\")\n",
    "    class_weights_tensor = torch.tensor(list(class_weights.values())).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    # TensorBoard\n",
    "    writer = SummaryWriter(log_dir=f\"./logs/{start_of_training}_{model_name}\")\n",
    "\n",
    "    # Resume from Checkpoint\n",
    "    initial_epoch = 0\n",
    "    if resume_from_checkpoint:\n",
    "        # Parse initial epoch from checkpoint filename\n",
    "        m = re.match(r'.*_(\\d+).pth', resume_from_checkpoint)\n",
    "        if m:\n",
    "            initial_epoch = int(m.group(1))\n",
    "        checkpoint = torch.load(resume_from_checkpoint)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Resumed training from checkpoint: {resume_from_checkpoint}, starting from epoch {initial_epoch}\")\n",
    "\n",
    "    print(training_configuration.summary())\n",
    "\n",
    "    # Early Stopping\n",
    "    early_stopping_counter = 0\n",
    "    best_val_accuracy = 0.0\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(initial_epoch, training_configuration.number_of_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            if bounding_boxes is not None:\n",
    "                images, labels, _ = batch \n",
    "            else:\n",
    "                images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_accuracy = 100.0 * correct / total\n",
    "        writer.add_scalar(\"Loss/Train\", running_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar(\"Accuracy/Train\", train_accuracy, epoch)\n",
    "        print(f\"Epoch {epoch + 1}/{training_configuration.number_of_epochs}, Train Loss: {running_loss / len(train_loader):.4f}, Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                if bounding_boxes is not None:\n",
    "                    images, labels, _ = batch\n",
    "                else:\n",
    "                    images, labels = batch\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_accuracy = 100.0 * correct / total\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss / len(val_loader), epoch)\n",
    "        writer.add_scalar(\"Accuracy/Validation\", val_accuracy, epoch)\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_accuracy)\n",
    "\n",
    "        # Early Stopping Logic\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            early_stopping_counter = 0\n",
    "            # Save best model\n",
    "            best_model_path = f\"{start_of_training.date()}_{model_name}_best.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, best_model_path)\n",
    "            print(f\"Saved best model to {best_model_path}\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= training_configuration.number_of_epochs_before_early_stopping:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "        # Save after every epoch if required\n",
    "        if save_after_every_epoch:\n",
    "            checkpoint_path = f\"{start_of_training.date()}_{model_name}_epoch_{epoch + 1}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, checkpoint_path)\n",
    "            print(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "\n",
    "    # Load the best model for testing\n",
    "    print(\"Loading best model for testing...\")\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    # Test Phase\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions, true_classes = [], []\n",
    "    file_names = []\n",
    "    misclassified_files = []\n",
    "    class_labels = test_dataset.classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            if bounding_boxes is not None:\n",
    "                images, labels, _ = batch\n",
    "            else:\n",
    "                images, labels = batch\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_classes.extend(labels.cpu().numpy())\n",
    "            # Get file names\n",
    "            batch_samples = test_dataset.samples[batch_idx * training_minibatch_size: (batch_idx + 1) * training_minibatch_size]\n",
    "            batch_file_names = [s[0] for s in batch_samples]\n",
    "            file_names.extend(batch_file_names)\n",
    "\n",
    "            # Record misclassified files\n",
    "            for idx in range(len(labels)):\n",
    "                if labels[idx] != predicted[idx]:\n",
    "                    misclassified_files.append((batch_file_names[idx], class_labels[predicted[idx]]))\n",
    "\n",
    "    classification_accuracy = 100.0 * correct / total\n",
    "    print(f\"Test Accuracy: {classification_accuracy:.2f}%\")\n",
    "\n",
    "    print(\"Reporting classification statistics with micro average\")\n",
    "    report = sklearn_reporting.classification_report(true_classes, predictions, digits=3,\n",
    "                                                     target_names=class_labels, average='micro')\n",
    "    print(report)\n",
    "\n",
    "    print(\"Reporting classification statistics with macro average\")\n",
    "    report = sklearn_reporting.classification_report(true_classes, predictions, digits=3,\n",
    "                                                     target_names=class_labels, average='macro')\n",
    "    print(report)\n",
    "\n",
    "    print(\"Reporting classification statistics with weighted average\")\n",
    "    report = sklearn_reporting.classification_report(true_classes, predictions, digits=3,\n",
    "                                                     target_names=class_labels, average='weighted')\n",
    "    print(report)\n",
    "\n",
    "    print(\"Misclassified files:\")\n",
    "    for file_path, predicted_class in misclassified_files:\n",
    "        print(f\"\\t{os.path.basename(file_path)} is incorrectly classified as {predicted_class}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    execution_time_in_seconds = round(end_time - start_time)\n",
    "    print(f\"Execution time: {execution_time_in_seconds:.1f}s\")\n",
    "\n",
    "    # Plot Training History\n",
    "    # Assuming TrainingHistoryPlotter can work with the logged data\n",
    "    # You might need to adjust this function to read data from TensorBoard logs\n",
    "    training_result_image = f\"{start_of_training.date()}_{training_configuration.name()}_{classification_accuracy:.1f}p.png\"\n",
    "    TrainingHistoryPlotter.plot_history(writer, training_result_image)\n",
    "\n",
    "    datasets_string = ','.join(datasets)\n",
    "    notification_message = f\"Training on {datasets_string} dataset with model {model_name} finished. Accuracy: {classification_accuracy:.5f}%\"\n",
    "    print(notification_message)\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "    parser.add_argument(\"--dataset_directory\", type=str, default=\"data\",\n",
    "                        help=\"The directory, that is used for storing the images during training\")\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"res_net_4\",\n",
    "                        help=\"The model used for training the network. Run ListAvailableConfigurations.ps1 or \"\n",
    "                             \"models/ConfigurationFactory.py to get a list of all available configurations\")\n",
    "\n",
    "    parser.add_argument(\"--use_existing_dataset_directory\", dest=\"delete_and_recreate_dataset_directory\",\n",
    "                        action='store_false',\n",
    "                        help=\"Whether to delete and recreate the dataset-directory (by downloading the appropriate \"\n",
    "                             \"files from the internet, extracting and generating images) or simply use whatever data \"\n",
    "                             \"currently is inside of that directory.\")\n",
    "    parser.set_defaults(delete_and_recreate_dataset_directory=True)\n",
    "\n",
    "    parser.add_argument(\"--minibatch_size\", default=16, type=int,\n",
    "                        help=\"Size of the minibatches for training, typically one of 8, 16, 32, 64 or 128\")\n",
    "    parser.add_argument(\"--optimizer\", default=\"Adadelta\", type=str,\n",
    "                        help=\"The optimizer used for the training, can be SGD, Adam or Adadelta\")\n",
    "\n",
    "    parser.add_argument(\"--no_dynamic_learning_rate_reduction\", dest=\"dynamic_learning_rate_reduction\",\n",
    "                        action=\"store_false\",\n",
    "                        help=\"True, if the learning rate should not be scheduled to be reduced on a plateau.\")\n",
    "    parser.set_defaults(dynamic_learning_rate_reduction=True)\n",
    "    parser.add_argument(\"--class_weights_balancing_method\", default=None, type=str,\n",
    "                        help=\"The optional weight balancing method for handling unbalanced datasets. If provided,\"\n",
    "                             \"valid choices are simple or skBalance. 'simple' uses 1/sqrt(#samples_per_class) as \"\n",
    "                             \"weights for samples from each class to compensate for classes that are underrepresented.\"\n",
    "                             \"'skBalance' uses the Python SkLearn module to calculate more sophisticated weights.\")\n",
    "    parser.add_argument(\"--save_after_every_epoch\", dest=\"save_after_every_epoch\", action=\"store_true\",\n",
    "                        help=\"Write a checkpoint after every epoch\")\n",
    "    parser.set_defaults(save_after_every_epoch=False)\n",
    "    parser.add_argument(\"--resume_from_checkpoint\", dest=\"resume_from_checkpoint\", default=None, type=str,\n",
    "                        help=\"Load checkpoint from file specified.\")\n",
    "\n",
    "    TrainingDatasetProvider.add_arguments_for_training_dataset_provider(parser)\n",
    "\n",
    "    flags, unparsed = parser.parse_known_args()\n",
    "\n",
    "    offsets = []\n",
    "    if flags.offsets != \"\":\n",
    "        offsets = [int(o) for o in flags.offsets.split(',')]\n",
    "    stroke_thicknesses_for_generated_symbols = [int(s) for s in flags.stroke_thicknesses.split(',')]\n",
    "\n",
    "    if flags.datasets == \"\":\n",
    "        raise Exception(\"No dataset selected. Specify the dataset for the training via the --dataset parameter\")\n",
    "    datasets = flags.datasets.split(',')\n",
    "\n",
    "    if flags.delete_and_recreate_dataset_directory:\n",
    "        training_dataset_provider = TrainingDatasetProvider(flags.dataset_directory)\n",
    "        training_dataset_provider.recreate_and_prepare_datasets_for_training(\n",
    "            datasets=datasets, width=flags.width,\n",
    "            height=flags.height,\n",
    "            use_fixed_canvas=flags.use_fixed_canvas,\n",
    "            stroke_thicknesses_for_generated_symbols=stroke_thicknesses_for_generated_symbols,\n",
    "            staff_line_spacing=flags.staff_line_spacing,\n",
    "            staff_line_vertical_offsets=offsets,\n",
    "            random_position_on_canvas=flags.random_position_on_canvas)\n",
    "        training_dataset_provider.resize_all_images_to_fixed_size(flags.width, flags.height)\n",
    "        training_dataset_provider.split_dataset_into_training_validation_and_test_set()\n",
    "\n",
    "    train_model(dataset_directory=flags.dataset_directory,\n",
    "                model_name=flags.model_name,\n",
    "                stroke_thicknesses=stroke_thicknesses_for_generated_symbols,\n",
    "                width=flags.width,\n",
    "                height=flags.height,\n",
    "                staff_line_vertical_offsets=offsets,\n",
    "                training_minibatch_size=flags.minibatch_size,\n",
    "                optimizer=flags.optimizer,\n",
    "                dynamic_learning_rate_reduction=flags.dynamic_learning_rate_reduction,\n",
    "                use_fixed_canvas=flags.use_fixed_canvas,\n",
    "                datasets=datasets,\n",
    "                class_weights_balancing_method=flags.class_weights_balancing_method,\n",
    "                save_after_every_epoch=flags.save_after_every_epoch,\n",
    "                resume_from_checkpoint=flags.resume_from_checkpoint,\n",
    "                send_telegram_messages=flags.send_telegram_messages)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
